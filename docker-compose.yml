# Local development scaffold for SystemUpdate-Web (M0)
# NOTE: Service definitions are minimal and may be extended in M1+.

services:
  # Main application PostgreSQL
  postgres:
    image: postgres:15-alpine
    container_name: su-postgres
    environment:
      POSTGRES_USER: systemupdate
      POSTGRES_PASSWORD: systemupdate
      POSTGRES_DB: systemupdate
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 20
      
  # Keycloak PostgreSQL
  keycloak-db:
    image: postgres:15-alpine
    container_name: su-keycloak-db
    environment:
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD: keycloak
      POSTGRES_DB: keycloak
    volumes:
      - keycloak_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U keycloak -d keycloak"]
      interval: 5s
      timeout: 3s
      retries: 20

  redis:
    image: redis:7-alpine
    container_name: su-redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 20

  zookeeper:
    image: bitnami/zookeeper:3.9
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    ports:
      - "2181:2181"

  prometheus:
    image: prom/prometheus:v2.53.0
    container_name: su-prometheus
    profiles: ["dev"]
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./observability/alerts.rules.yml:/etc/prometheus/alerts.rules.yml:ro
      - ./observability/recording.rules.yml:/etc/prometheus/recording.rules.yml:ro
    ports:
      - "9090:9090"
    depends_on:
      otel-collector:
        condition: service_started

  grafana:
    image: grafana/grafana:11.1.0
    container_name: su-grafana
    profiles: ["dev"]
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GRAFANA_SLACK_WEBHOOK_URL: ${GRAFANA_SLACK_WEBHOOK_URL:-}
    ports:
      - "3000:3000"
    volumes:
      - ./observability/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./observability/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./observability/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./observability/grafana/provisioning/alerting:/etc/grafana/provisioning/alerting:ro
    depends_on:
      prometheus:
        condition: service_started
      tempo:
        condition: service_started

  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: su-alertmanager
    profiles: ["dev"]
    command: ["--config.file=/etc/alertmanager/alertmanager.yml"]
    ports:
      - "9093:9093"
    volumes:
      - ./observability/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro

  blackbox-exporter:
    image: prom/blackbox-exporter:v0.25.0
    container_name: su-blackbox
    profiles: ["dev"]
    ports:
      - "9115:9115"

  kafka:
    image: bitnami/kafka:3.7
    environment:
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      ALLOW_PLAINTEXT_LISTENER: "yes"
      KAFKA_LISTENERS: PLAINTEXT://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    depends_on:
      zookeeper:
        condition: service_started
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12

  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.1
    container_name: su-schema-registry
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8081/subjects >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12

  minio:
    image: minio/minio:latest
    profiles: ["dev"]
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9000/minio/health/ready >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12

  auth-service:
    build: ./services/auth-service
    container_name: su-auth
    environment:
      PORT: 8001
      OTEL_TRACES_ENABLE: "1"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_SERVICE_NAME: auth-service
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8001"]
    ports:
      - "8001:8001"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8001/healthz"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s

  notification-service:
    build: ./services/notification-service
    container_name: su-notification
    environment:
      PORT: 8007
      OTEL_TRACES_ENABLE: "1"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_SERVICE_NAME: notification-service
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8007"]
    ports:
      - "8007:8007"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8007/healthz"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    depends_on:
      otel-collector:
        condition: service_started

  ws-hub:
    build: ./services/ws-hub
    container_name: su-ws-hub
    environment:
      PORT: 8002
      OTEL_TRACES_ENABLE: "1"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_SERVICE_NAME: ws-hub
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8002"]
    ports:
      - "8002:8002"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8002/healthz"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    depends_on:
      redis:
        condition: service_healthy

  device-service:
    build: ./services/device-service
    container_name: su-device
    environment:
      PORT: 8003
      OTEL_TRACES_ENABLE: "1"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_SERVICE_NAME: device-service
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8003"]
    ports:
      - "8003:8003"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8003/healthz"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    depends_on:
      postgres:
        condition: service_healthy

  command-service:
    build: ./services/command-service
    container_name: su-command
    environment:
      PORT: 8004
      KAFKA_BOOTSTRAP: kafka:9092
      COMMAND_EVENTS_TOPIC: command.events
      POSTGRES_HOST: postgres
      POSTGRES_PORT: "5432"
      POSTGRES_USER: systemupdate
      POSTGRES_PASSWORD: systemupdate
      POSTGRES_DB: systemupdate
      OTEL_TRACES_ENABLE: "1"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_SERVICE_NAME: command-service
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8004"]
    ports:
      - "8004:8004"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8004/healthz"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  data-ingest-service:
    build: ./services/data-ingest-service
    container_name: su-data-ingest
    environment:
      PORT: 8005
      KAFKA_BOOTSTRAP: kafka:9092
      INGEST_TOPIC: device.ingest.raw
      OTEL_TRACES_ENABLE: "1"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_SERVICE_NAME: data-ingest-service
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8005"]
    ports:
      - "8005:8005"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8005/healthz"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    depends_on:
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy

  analytics-service:
    build: ./services/analytics-service
    container_name: su-analytics
    environment:
      PORT: 8006
      OTEL_TRACES_ENABLE: "1"
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_SERVICE_NAME: analytics-service
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8006"]
    ports:
      - "8006:8006"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8006/healthz >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
    depends_on:
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy

  gateway:
    image: kong:3.6
    container_name: su-gateway
    profiles: ["dev"]
    environment:
      KONG_DATABASE: off
      KONG_DECLARATIVE_CONFIG: /etc/kong/kong.yml
      KONG_LOG_LEVEL: info
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_PROXY_LISTEN: 0.0.0.0:8000
      # CORS plugin can be enabled per-route later; leave global defaults minimal
    ports:
      - "8000:8000"  # proxy
      - "8001:8001"  # admin api
    volumes:
      - ./gateway/kong.yml:/etc/kong/kong.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8001/status >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
    depends_on:
      auth-service:
        condition: service_healthy
      device-service:
        condition: service_healthy
      command-service:
        condition: service_healthy
      analytics-service:
        condition: service_healthy
      notification-service:
        condition: service_healthy
      ws-hub:
        condition: service_healthy

  # Secure gateway profile (tight CORS, per-route plugins)
  gateway-secure:
    image: kong:3.6
    container_name: su-gateway-secure
    profiles: ["secure"]
    environment:
      KONG_DATABASE: off
      KONG_DECLARATIVE_CONFIG: /etc/kong/kong.yml
      KONG_LOG_LEVEL: info
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: off
      KONG_PROXY_LISTEN: 0.0.0.0:8000
    ports:
      - "8000:8000"
    volumes:
      - ./gateway/kong.prod.yml:/etc/kong/kong.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "(curl -s -o /dev/null -w '%{http_code}' http://localhost:8000 || echo 000) | grep -E '200|404' >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
    depends_on:
      auth-service:
        condition: service_healthy
      device-service:
        condition: service_healthy
      command-service:
        condition: service_healthy
      analytics-service:
        condition: service_healthy
      notification-service:
        condition: service_healthy
      ws-hub:
        condition: service_healthy

  # Optional: Keycloak for OIDC dev profile (enable with: docker compose --profile auth up -d keycloak)
  keycloak-db:
    image: postgres:15-alpine
    container_name: su-keycloak-db
    environment:
      POSTGRES_DB: keycloak
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD: keycloak
    volumes:
      - keycloak_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U keycloak -d keycloak"]
      interval: 5s
      timeout: 5s
      retries: 5

  keycloak:
    image: quay.io/keycloak/keycloak:24.0.2
    container_name: su-keycloak
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://keycloak-db:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: keycloak
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      KC_HOSTNAME: localhost
      KC_HTTP_ENABLED: 'true'
      KC_PROXY: edge
      KC_HOSTNAME_STRICT: 'false'
    command: ["start-dev"]
    ports:
      - "8080:8080"
    depends_on:
      keycloak-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dev"]

  # Optional: OPA for policy decisions and decision logs (enable with: docker compose --profile policy up -d opa)
  opa:
    image: openpolicyagent/opa:latest
    container_name: su-opa
    profiles: ["policy"]
    command: ["run", "--server", "--addr=0.0.0.0:8181", "--set=decision_logs.console=true", "/policy"]
    ports:
      - "8181:8181"
    volumes:
      - ./auth/opa:/policy:ro

  # Optional: OPAL server and client for policy bundle sync (demo)
  opal-server:
    image: permitio/opal-server:latest
    container_name: su-opal-server
    profiles: ["policy"]
    environment:
      OPAL_LOG_LEVEL: INFO
      # Configure Git policy repo (change these to your own repo/branch)
      OPAL_POLICY_REPO_URL: ${OPAL_POLICY_REPO_URL:-https://github.com/nodweb/systemupdate-policies.git}
      OPAL_POLICY_REPO_BRANCH: ${OPAL_POLICY_REPO_BRANCH:-main}
      OPAL_POLICY_REPO_POLLING_INTERVAL: ${OPAL_POLICY_REPO_POLLING_INTERVAL:-30}
      # Optional: path within repo for policies/bundles
      OPAL_POLICY_REPO_PATH: ${OPAL_POLICY_REPO_PATH:-/}
    ports:
      - "7002:7002"
    depends_on:
      opa:
        condition: service_started

  opal-client:
    image: permitio/opal-client:latest
    container_name: su-opal-client
    profiles: ["policy"]
    environment:
      OPAL_LOG_LEVEL: INFO
      OPAL_CLIENT_POLICY_STORE_URL: http://opa:8181
      OPAL_CLIENT_SERVER_URL: http://opal-server:7002
    depends_on:
      opa:
        condition: service_started
      opal-server:
        condition: service_started

  otel-collector:
    image: otel/opentelemetry-collector:0.95.0
    container_name: su-otel-collector
    volumes:
      - ./observability/otel-collector-config.yaml:/etc/otel/config.yaml:ro
    command: ["--config", "/etc/otel/config.yaml"]
    ports:
      - "4317:4317"  # OTLP/gRPC
      - "4318:4318"  # OTLP/HTTP

  tempo:
    image: grafana/tempo:2.6.0
    container_name: su-tempo
    profiles: ["dev"]
    volumes:
      - ./observability/tempo.yaml:/etc/tempo/tempo.yaml:ro
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    ports:
      - "3200:3200"   # Tempo Query (HTTP)
      - "4319:4317"   # OTLP/gRPC (host 4319 -> container 4317 to avoid conflict)
      - "4320:4318"   # OTLP/HTTP (host 4320 -> container 4318)


volumes:
  pgdata:
  minio:
  keycloak_db_data:
