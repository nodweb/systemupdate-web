groups:
  - name: systemupdate-route-slo-alerts
    interval: 30s
    rules:
      - alert: RouteLowAvailability
        expr: route:availability:percent5m{http_route!~"^/(health|metrics)(/.*)?$"} < 95
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low availability (<95%) on route {{ $labels.service_name }} {{ $labels.http_route }}"
          description: |
            Route availability dropped below 95% over 5m window.
            current={{ $value }}% service={{ $labels.service_name }} route={{ $labels.http_route }}

      - alert: RouteVeryLowAvailability
        expr: route:availability:percent5m{http_route!~"^/(health|metrics)(/.*)?$"} < 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Very low availability (<90%) on route {{ $labels.service_name }} {{ $labels.http_route }}"
          description: |
            Route availability dropped below 90% over 5m window.
            current={{ $value }}% service={{ $labels.service_name }} route={{ $labels.http_route }}

      - alert: RouteHighLatencyP95
        expr: route:latency_p95_ms:5m{http_route!~"^/(health|metrics)(/.*)?$"} > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High p95 latency (>500ms) on route {{ $labels.service_name }} {{ $labels.http_route }}"
          description: |
            Route p95 latency exceeded 500ms over the last 5 minutes.
            p95_ms={{ $value }} service={{ $labels.service_name }} route={{ $labels.http_route }}

      # Route-level burn rate alerts for 99.5% SLO (budget 0.005)
      - alert: RouteFastBurnRate
        expr: route:error_rate:ratio_rate5m{http_route!~"^/(health|metrics)(/.*)?$"} > (14 * 0.005)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Fast burn rate on route {{ $labels.service_name }} {{ $labels.http_route }} (>14x over 5m)"
          description: |
            Route is burning error budget rapidly.
            error_rate_5m={{ $value }} budget={{ 0.005 }} service={{ $labels.service_name }} route={{ $labels.http_route }}

      - alert: RouteSlowBurnRate
        expr: |
          (
            sum by(service_name, http_route)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK", http_route!~"^/(health|metrics)(/.*)?$"}[1h]))
            /
            clamp_min(sum by(service_name, http_route)(rate(traces_spanmetrics_calls_total{http_route!~"^/(health|metrics)(/.*)?$"}[1h])), 1e-9)
          ) > (6 * 0.005)
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Slow burn rate on route {{ $labels.service_name }} {{ $labels.http_route }} (>6x over 1h)"
          description: |
            Route is consuming error budget steadily.
            error_rate_1h={{ $value }} budget={{ 0.005 }} service={{ $labels.service_name }} route={{ $labels.http_route }}
  - name: systemupdate-service-alerts
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: |
          (
            sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[5m]))
            /
            clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[5m])), 1e-9)
          ) > 0.05

      # Burn rate alerts for 99.5% SLO (error budget = 0.5% => 0.005)
      # Fast burn: 5m window > 14x budget (~consume budget in ~1h)
      - alert: ServiceFastBurnRate
        expr: service:error_rate:ratio_rate5m > (14 * 0.005)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Fast burn rate on {{ $labels.service_name }} (>14x budget over 5m)"
          description: |
            Service is burning error budget rapidly.
            error_rate_5m={{ $value }} budget={{ 0.005 }} service={{ $labels.service_name }}

      # Slow burn: 1h window > 6x budget (~consume budget in few hours)
      - alert: ServiceSlowBurnRate
        expr: |
          (
            sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[1h]))
            /
            clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[1h])), 1e-9)
          ) > (6 * 0.005)
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Slow burn rate on {{ $labels.service_name }} (>6x budget over 1h)"
          description: |
            Service is consuming error budget steadily.
            error_rate_1h={{ $value }} budget={{ 0.005 }} service={{ $labels.service_name }}
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate (>5%) for service {{ $labels.service_name }}"
          description: |
            Error rate for {{ $labels.service_name }} exceeded 5% over the last 5 minutes.
            value={{ $value }}

      - alert: VeryHighErrorRate
        expr: |
          (
            sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[5m]))
            /
            clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[5m])), 1e-9)
          ) > 0.15
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Very high error rate (>15%) for service {{ $labels.service_name }}"
          description: |
            Error rate for {{ $labels.service_name }} exceeded 15% over the last 5 minutes.
            value={{ $value }}

      - alert: HighLatencyP95
        expr: |
          histogram_quantile(0.95, sum by (le, service_name) (rate(traces_spanmetrics_duration_milliseconds_bucket[5m]))) > 500
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High p95 latency (>500ms) for service {{ $labels.service_name }}"
          description: |
            p95 latency for {{ $labels.service_name }} exceeded 500ms over the last 5 minutes.
            value={{ $value }} ms

      # Error budget burn alerts for 95% availability (5% error budget)
      # Fast-burn (1h/5m) using multiplier ~14.4
      - alert: ErrorBudgetBurnFast
        expr: |
          (
            (
              sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[5m]))
              /
              clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[5m])), 1e-9)
            ) / 0.05
          ) > 14.4
          and
          (
            (
              sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[1h]))
              /
              clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[1h])), 1e-9)
            ) / 0.05
          ) > 14.4
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Fast error budget burn for {{ $labels.service_name }}"
          description: |
            {{ $labels.service_name }} is burning error budget quickly (5m and 1h both >14.4x budget for 95% SLO).
            5m burn rate={{ $value }}x (note: shows current series value); verify alongside 1h window in Prometheus.

      # Slow-burn (6h/30m) using multiplier ~6
      - alert: ErrorBudgetBurnSlow
        expr: |
          (
            (
              sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[30m]))
              /
              clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[30m])), 1e-9)
            ) / 0.05
          ) > 6
          and
          (
            (
              sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[6h]))
              /
              clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[6h])), 1e-9)
            ) / 0.05
          ) > 6
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Slow error budget burn for {{ $labels.service_name }}"
          description: |
            {{ $labels.service_name }} is burning error budget steadily (30m and 6h both >6x budget for 95% SLO).

  - name: systemupdate-uptime-alerts
    interval: 30s
    rules:
      - alert: EndpointDownWarning
        expr: probe_success{job="blackbox-http"} == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Endpoint down (probe failed): {{ $labels.instance }}"
          description: |
            Blackbox probe failed for {{ $labels.instance }} for at least 1 minute.

      - alert: EndpointDownCritical
        expr: probe_success{job="blackbox-http"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint down (critical): {{ $labels.instance }}"
          description: |
            Blackbox probe has been failing for {{ $labels.instance }} for 5 minutes.

      - alert: EndpointHighLatency
        expr: avg_over_time(probe_duration_seconds{job="blackbox-http"}[5m]) > 1.0
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "High probe latency (>1s avg over 5m): {{ $labels.instance }}"
          description: |
            Average probe duration for {{ $labels.instance }} exceeded 1s over the last 5 minutes.
