groups:
  # Service Health Alerts
  - name: systemupdate-service-health
    rules:
      # Service Down Alerts
      - alert: ServiceDown
        expr: up{job=~"services-metrics|blackbox-http"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'Service {{ $labels.instance }} is down'
          description: 'Service {{ $labels.instance }} has been down for more than 1 minute.'

      # High Error Rate Alerts
      - alert: HighErrorRate
        expr: rate(http_requests_total{job=~"services-metrics", status=~"5.."}[5m]) / rate(http_requests_total{job=~"services-metrics"}[5m]) > 0.01
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High error rate on {{ $labels.service }}'
          description: 'Error rate is {{ $value }}% for service {{ $labels.service }} (instance {{ $labels.instance }})'

      # High Latency Alerts
      - alert: HighLatency
        expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=~"services-metrics"}[5m])) by (le, service, path, method)) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: 'High latency on {{ $labels.service }} {{ $labels.method }} {{ $labels.path }}'
          description: 'p95 latency is {{ $value }}s for service {{ $labels.service }} {{ $labels.method }} {{ $labels.path }}'

      # Database Connection Alerts
      - alert: DatabaseHighConnections
        expr: pg_stat_activity_count > pg_settings_max_connections * 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High database connections on {{ $labels.instance }}'
          description: 'Database connections ({{ $value }}) exceed 80% of max connections'

      # Kafka Consumer Lag Alerts
      - alert: KafkaConsumerLagHigh
        expr: kafka_consumergroup_lag > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High consumer lag for {{ $labels.consumergroup }} on {{ $labels.topic }}'
          description: 'Consumer lag is {{ $value }} messages for group {{ $labels.consumergroup }} on topic {{ $labels.topic }}'

      # WebSocket Connection Alerts
      - alert: WebSocketConnectionsDropped
        expr: rate(websocket_connections_closed_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High rate of WebSocket disconnections on {{ $labels.instance }}'
          description: '{{ $value }} WebSocket connections dropped per second'

  # Infrastructure Health Alerts
  - name: systemupdate-infrastructure-health
    rules:
      # Node Exporter Down
      - alert: NodeExporterDown
        expr: up{job="node"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: 'Node Exporter down on {{ $labels.instance }}'
          description: 'Node Exporter has been down for more than 5 minutes.'

      # High CPU Usage
      - alert: HighCpuUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: 'High CPU usage on {{ $labels.instance }}'
          description: 'CPU usage is {{ $value }}%'

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / (node_memory_MemTotal_bytes))) * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: 'High memory usage on {{ $labels.instance }}'
          description: 'Memory usage is {{ $value }}%'

      # High Disk Usage
      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes{mountpoint=~"/|/var/lib/docker"} / node_filesystem_size_bytes{mountpoint=~"/|/var/lib/docker"})) * 100 > 85
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: 'High disk usage on {{ $labels.instance }} {{ $labels.mountpoint }}'
          description: 'Disk usage is {{ $value }}%'

  # Blackbox Exporter Alerts
  - name: systemupdate-blackbox-alerts
    rules:
      # HTTP Probe Failed
      - alert: ProbeHttpFailure
        expr: probe_success{job="blackbox-http"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'HTTP probe failed for {{ $labels.instance }}'
          description: 'HTTP probe failed for {{ $labels.instance }}'

      # High Response Time
      - alert: ProbeHighResponseTime
        expr: probe_http_duration_seconds{job="blackbox-http"} > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High response time for {{ $labels.instance }}'
          description: 'Response time is {{ $value }}s for {{ $labels.instance }}'

  # Route SLO Alerts
  - name: systemupdate-route-slo-alerts
    interval: 30s
    rules:
      - alert: RouteLowAvailability
        expr: route:availability:percent5m{http_route!~"^/(health|metrics)(/.*)?$"} < 95
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low availability (<95%) on route {{ $labels.service_name }} {{ $labels.http_route }}"
          description: |
            Route availability dropped below 95% over 5m window.
            current={{ $value }}% service={{ $labels.service_name }} route={{ $labels.http_route }}

      - alert: RouteVeryLowAvailability
        expr: route:availability:percent5m{http_route!~"^/(health|metrics)(/.*)?$"} < 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Very low availability (<90%) on route {{ $labels.service_name }} {{ $labels.http_route }}"
          description: |
            Route availability dropped below 90% over 5m window.
            current={{ $value }}% service={{ $labels.service_name }} route={{ $labels.http_route }}

      - alert: RouteHighLatencyP95
        expr: route:latency_p95_ms:5m{http_route!~"^/(health|metrics)(/.*)?$"} > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High p95 latency (>500ms) on route {{ $labels.service_name }} {{ $labels.http_route }}"
          description: |
            Route p95 latency exceeded 500ms over the last 5 minutes.
            p95_ms={{ $value }} service={{ $labels.service_name }} route={{ $labels.http_route }}

      # Route-level burn rate alerts for 99.5% SLO (budget 0.005)
      - alert: RouteFastBurnRate
        expr: route:error_rate:ratio_rate5m{http_route!~"^/(health|metrics)(/.*)?$"} > (14 * 0.005)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Fast burn rate on route {{ $labels.service_name }} {{ $labels.http_route }} (>14x over 5m)"
          description: |
            Route is burning error budget rapidly.
            error_rate_5m={{ $value }} budget={{ 0.005 }} service={{ $labels.service_name }} route={{ $labels.http_route }}

      - alert: RouteSlowBurnRate
        expr: |
          (
            sum by(service_name, http_route)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK", http_route!~"^/(health|metrics)(/.*)?$"}[1h]))
            /
            clamp_min(sum by(service_name, http_route)(rate(traces_spanmetrics_calls_total{http_route!~"^/(health|metrics)(/.*)?$"}[1h])), 1e-9)
          ) > (6 * 0.005)
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Slow burn rate on route {{ $labels.service_name }} {{ $labels.http_route }} (>6x over 1h)"
          description: |
            Route is consuming error budget steadily.
            error_rate_1h={{ $value }} budget={{ 0.005 }} service={{ $labels.service_name }} route={{ $labels.http_route }}
  - name: systemupdate-service-alerts
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: |
          (
            sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[5m]))
            /
            clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[5m])), 1e-9)
          ) > 0.05

      # Burn rate alerts for 99.5% SLO (error budget = 0.5% => 0.005)
      # Fast burn: 5m window > 14x budget (~consume budget in ~1h)
      - alert: ServiceFastBurnRate
        expr: service:error_rate:ratio_rate5m > (14 * 0.005)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Fast burn rate on {{ $labels.service_name }} (>14x budget over 5m)"
          description: |
            Service is burning error budget rapidly.
            error_rate_5m={{ $value }} budget={{ 0.005 }} service={{ $labels.service_name }}

      # Slow burn: 1h window > 6x budget (~consume budget in few hours)
      - alert: ServiceSlowBurnRate
        expr: |
          (
            sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[1h]))
            /
            clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[1h])), 1e-9)
          ) > (6 * 0.005)
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Slow burn rate on {{ $labels.service_name }} (>6x budget over 1h)"
          description: |
            Service is consuming error budget steadily.
            error_rate_1h={{ $value }} budget={{ 0.005 }} service={{ $labels.service_name }}
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate (>5%) for service {{ $labels.service_name }}"
          description: |
            Error rate for {{ $labels.service_name }} exceeded 5% over the last 5 minutes.
            value={{ $value }}

      - alert: VeryHighErrorRate
        expr: |
          (
            sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[5m]))
            /
            clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[5m])), 1e-9)
          ) > 0.15
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Very high error rate (>15%) for service {{ $labels.service_name }}"
          description: |
            Error rate for {{ $labels.service_name }} exceeded 15% over the last 5 minutes.
            value={{ $value }}

      - alert: HighLatencyP95
        expr: |
          histogram_quantile(0.95, sum by (le, service_name) (rate(traces_spanmetrics_duration_milliseconds_bucket[5m]))) > 500
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High p95 latency (>500ms) for service {{ $labels.service_name }}"
          description: |
            p95 latency for {{ $labels.service_name }} exceeded 500ms over the last 5 minutes.
            value={{ $value }} ms

      # Error budget burn alerts for 95% availability (5% error budget)
      # Fast-burn (1h/5m) using multiplier ~14.4
      - alert: ErrorBudgetBurnFast
        expr: |
          (
            (
              sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[5m]))
              /
              clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[5m])), 1e-9)
            ) / 0.05
          ) > 14.4
          and
          (
            (
              sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[1h]))
              /
              clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[1h])), 1e-9)
            ) / 0.05
          ) > 14.4
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Fast error budget burn for {{ $labels.service_name }}"
          description: |
            {{ $labels.service_name }} is burning error budget quickly (5m and 1h both >14.4x budget for 95% SLO).
            5m burn rate={{ $value }}x (note: shows current series value); verify alongside 1h window in Prometheus.

      # Slow-burn (6h/30m) using multiplier ~6
      - alert: ErrorBudgetBurnSlow
        expr: |
          (
            (
              sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[30m]))
              /
              clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[30m])), 1e-9)
            ) / 0.05
          ) > 6
          and
          (
            (
              sum by(service_name)(rate(traces_spanmetrics_calls_total{status_code!="STATUS_CODE_OK"}[6h]))
              /
              clamp_min(sum by(service_name)(rate(traces_spanmetrics_calls_total[6h])), 1e-9)
            ) / 0.05
          ) > 6
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Slow error budget burn for {{ $labels.service_name }}"
          description: |
            {{ $labels.service_name }} is burning error budget steadily (30m and 6h both >6x budget for 95% SLO).

  - name: systemupdate-uptime-alerts
    interval: 30s
    rules:
      - alert: EndpointDownWarning
        expr: probe_success{job="blackbox-http"} == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Endpoint down (probe failed): {{ $labels.instance }}"
          description: |
            Blackbox probe failed for {{ $labels.instance }} for at least 1 minute.

      - alert: EndpointDownCritical
        expr: probe_success{job="blackbox-http"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint down (critical): {{ $labels.instance }}"
          description: |
            Blackbox probe has been failing for {{ $labels.instance }} for 5 minutes.

      - alert: EndpointHighLatency
        expr: avg_over_time(probe_duration_seconds{job="blackbox-http"}[5m]) > 1.0
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "High probe latency (>1s avg over 5m): {{ $labels.instance }}"
          description: |
            Average probe duration for {{ $labels.instance }} exceeded 1s over the last 5 minutes.
